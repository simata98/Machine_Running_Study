{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import itertools \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list()\n",
    "labels = list()\n",
    "\n",
    "with open(Path('./output/benign/all_benign.txt'), 'r') as f:\n",
    "  lines = f.read().split('\\n')[:-1]\n",
    "  # print(lines[0][33:])\n",
    "for line in lines:\n",
    "  doc = line[33:]\n",
    "  corpus.append(doc)\n",
    "  labels.append(0)\n",
    "\n",
    "with open(Path('./output/malware/all_malware.txt'), 'r') as f:\n",
    "  lines = f.read().split('\\n')[:-1]\n",
    "  # print(lines[0][33:])\n",
    "for line in lines:\n",
    "  doc = line[33:]\n",
    "  corpus.append(doc)\n",
    "  labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv3 = CountVectorizer(ngram_range=(3,3))\n",
    "X3 = cv3.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 안한 데이터셋 test_size 10%\n",
    "X31_train, X31_test, y31_train, y31_test = train_test_split(X3, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 정규화 하고 test_size 10%\n",
    "v3=np.array(X3).astype(np.float32)\n",
    "for i in range(len(corpus)):\n",
    "    s=sum(X3[i])\n",
    "    v3[i]=((X3[i]/s)*100).astype(np.float32)\n",
    "    \n",
    "X32_train, X32_test, y32_train, y32_test = train_test_split(v3, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RadomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150   2]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9434    0.9868    0.9646       152\n",
      "           1     0.9835    0.9297    0.9558       128\n",
      "\n",
      "    accuracy                         0.9607       280\n",
      "   macro avg     0.9634    0.9583    0.9602       280\n",
      "weighted avg     0.9617    0.9607    0.9606       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifierrf32 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)\n",
    "classifierrf32.fit(X32_train, y32_train)\n",
    "yrf32_pred=classifierrf32.predict(X32_test)\n",
    "\n",
    "cm32 = confusion_matrix(y32_test, yrf32_pred)\n",
    "print(cm32)\n",
    "print(metrics.classification_report(y32_test, yrf32_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148   4]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9427    0.9737    0.9579       152\n",
      "           1     0.9675    0.9297    0.9482       128\n",
      "\n",
      "    accuracy                         0.9536       280\n",
      "   macro avg     0.9551    0.9517    0.9531       280\n",
      "weighted avg     0.9540    0.9536    0.9535       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifierrf32 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)\n",
    "classifierrf32.fit(X31_train, y31_train)\n",
    "yrf31_pred=classifierrf32.predict(X31_test)\n",
    "\n",
    "cm31 = confusion_matrix(y31_test, yrf31_pred)\n",
    "print(cm31)\n",
    "print(metrics.classification_report(y31_test, yrf31_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화를 했을 때 더 성능이 높게 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   8]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9412    0.9474    0.9443       152\n",
      "           1     0.9370    0.9297    0.9333       128\n",
      "\n",
      "    accuracy                         0.9393       280\n",
      "   macro avg     0.9391    0.9385    0.9388       280\n",
      "weighted avg     0.9393    0.9393    0.9393       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifierknn32 = KNeighborsClassifier(n_neighbors = 2)\n",
    "classifierknn32.fit(X32_train, y32_train)\n",
    "\n",
    "yknn32_pred=classifierknn32.predict(X32_test)\n",
    "cm32 = confusion_matrix(y32_test, yknn32_pred)\n",
    "print(cm32)\n",
    "print(classification_report(y32_test, yknn32_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화한게 더 성능이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140  12]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9396    0.9211    0.9302       152\n",
      "           1     0.9084    0.9297    0.9189       128\n",
      "\n",
      "    accuracy                         0.9250       280\n",
      "   macro avg     0.9240    0.9254    0.9246       280\n",
      "weighted avg     0.9253    0.9250    0.9251       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifierknn31 = KNeighborsClassifier(n_neighbors = 2)\n",
    "classifierknn31.fit(X31_train, y31_train)\n",
    "\n",
    "yknn31_pred=classifierknn31.predict(X31_test)\n",
    "cm31 = confusion_matrix(y31_test, yknn31_pred)\n",
    "print(cm31)\n",
    "print(classification_report(y31_test, yknn31_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[134  18]\n",
      " [ 10 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9306    0.8816    0.9054       152\n",
      "           1     0.8676    0.9219    0.8939       128\n",
      "\n",
      "    accuracy                         0.9000       280\n",
      "   macro avg     0.8991    0.9017    0.8997       280\n",
      "weighted avg     0.9018    0.9000    0.9002       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# criterion gini가 entropy\n",
    "classifierdt32 = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\n",
    "\n",
    "classifierdt32.fit(X32_train, y32_train)\n",
    "ydt32_pred=classifierdt32.predict(X32_test)\n",
    "\n",
    "cm32 = confusion_matrix(y32_test, ydt32_pred)\n",
    "print(cm32)\n",
    "print(metrics.classification_report(y32_test, ydt32_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블 알고리즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c4nd0it/anaconda3/envs/keras_env/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c4nd0it/anaconda3/envs/keras_env/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:01:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1644955194972/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[[148   4]\n",
      " [ 12 116]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9250    0.9737    0.9487       152\n",
      "           1     0.9667    0.9062    0.9355       128\n",
      "\n",
      "    accuracy                         0.9429       280\n",
      "   macro avg     0.9458    0.9400    0.9421       280\n",
      "weighted avg     0.9440    0.9429    0.9427       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_XGB = XGBClassifier(eval_metric = 'logloss', \n",
    "                              booster = 'gbtree',\n",
    "                              colsample_bylevel=0.9,\n",
    "                              colsample_bytree=0.8,\n",
    "                              silent = True, \n",
    "                              max_depth = 6, \n",
    "                              min_child_weight = 1, \n",
    "                               gamma = 0, \n",
    "                               n_estimators=50,\n",
    "                               nthread=4,\n",
    "                               objective='binary:logistic',\n",
    "                               random_state=42,\n",
    "                               learning_rate=0.0001,\n",
    "                               )\n",
    "\n",
    "best_model_XGB.fit(X32_train, y32_train)\n",
    "XGB_pred=best_model_XGB.predict(X32_test)\n",
    "\n",
    "cm32 = confusion_matrix(y32_test, XGB_pred)\n",
    "print(cm32)\n",
    "print(metrics.classification_report(y32_test, XGB_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146   6]\n",
      " [  5 123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9669    0.9605    0.9637       152\n",
      "           1     0.9535    0.9609    0.9572       128\n",
      "\n",
      "    accuracy                         0.9607       280\n",
      "   macro avg     0.9602    0.9607    0.9604       280\n",
      "weighted avg     0.9608    0.9607    0.9607       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "model_LGBM = LGBMClassifier(boosting_type='gbdt',learning_rate=0.01 ,\n",
    "                            n_estimators=600, max_bin=255, \n",
    "                            num_leaves=24,random_state=42)\n",
    "\n",
    "model_LGBM.fit(X32_train, y32_train)\n",
    "LGBM_pred=model_LGBM.predict(X32_test)\n",
    "\n",
    "cm32 = confusion_matrix(y32_test, LGBM_pred)\n",
    "print(cm32)\n",
    "print(metrics.classification_report(y32_test, LGBM_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151   1]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9437    0.9934    0.9679       152\n",
      "           1     0.9917    0.9297    0.9597       128\n",
      "\n",
      "    accuracy                         0.9643       280\n",
      "   macro avg     0.9677    0.9616    0.9638       280\n",
      "weighted avg     0.9657    0.9643    0.9642       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_Extra = ExtraTreesClassifier(max_depth=25, n_estimators=320)\n",
    "\n",
    "\n",
    "model_Extra.fit(X32_train, y32_train)\n",
    "Extra_pred=model_Extra.predict(X32_test)\n",
    "\n",
    "cm32 = confusion_matrix(y32_test, Extra_pred)\n",
    "print(cm32)\n",
    "print(metrics.classification_report(y32_test, Extra_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftVoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c4nd0it/anaconda3/envs/keras_env/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:04:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1644955194972/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[[144   8]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9487    0.9737    0.9610       152\n",
      "           1     0.9677    0.9375    0.9524       128\n",
      "\n",
      "    accuracy                         0.9571       280\n",
      "   macro avg     0.9582    0.9556    0.9567       280\n",
      "weighted avg     0.9574    0.9571    0.9571       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "softVoting_model = VotingClassifier(estimators=[('RF', classifierrf32), ('KNN', classifierknn32), ('EXTRA', model_Extra), ('LGBM', model_LGBM), ('XGB', best_model_XGB)], voting='soft')\n",
    "softVoting_model.fit(X32_train, y32_train)\n",
    "\n",
    "soft_pred = softVoting_model.predict(X32_test)\n",
    "print(cm32)\n",
    "print(classification_report(y32_test, soft_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HardVoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c4nd0it/anaconda3/envs/keras_env/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:15:22] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1644955194972/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[[144   8]\n",
      " [  9 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9497    0.9934    0.9711       152\n",
      "           1     0.9917    0.9375    0.9639       128\n",
      "\n",
      "    accuracy                         0.9679       280\n",
      "   macro avg     0.9707    0.9655    0.9675       280\n",
      "weighted avg     0.9689    0.9679    0.9678       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hardVoting_model = VotingClassifier(estimators=[('RF', classifierrf32), ('KNN', classifierknn32), ('EXTRA', model_Extra), ('LGBM', model_LGBM), ('XGB', best_model_XGB)], voting='hard')\n",
    "hardVoting_model.fit(X32_train, y32_train)\n",
    "\n",
    "hard_pred = hardVoting_model.predict(X32_test)\n",
    "print(cm32)\n",
    "print(classification_report(y32_test, hard_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('keras_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263279b5c7f5294ef7179eeb038e9113bb5dce42ff0c0d19b3e81ce1438dd79a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
